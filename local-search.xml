<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>K8s</title>
    <link href="/2024/03/22/K8s/"/>
    <url>/2024/03/22/K8s/</url>
    
    <content type="html"><![CDATA[<p>Testing</p><span id="more"></span><h2 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h2>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hypervisor介绍</title>
    <link href="/2022/03/10/hypervisor%E4%BB%8B%E7%BB%8D/"/>
    <url>/2022/03/10/hypervisor%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<p>hypervisor作用、类型和实现简介。</p><span id="more"></span><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>hypervisor又称VMM（virtual machine monitor），是一种用来创建、管理虚拟机的软件程序。运行hypervisor的硬件设施被称为宿主机（母机），在这之上可以使用hypervisor虚拟出来更多的子机，每个子机都有自己独立的操作系统（guest os）。</p><p>hypervisor把它管理的宿主机硬件设施资源如CPU、内存、磁盘存储等当做一个资源池子，按照一定策略分配给运行在其上的虚拟机。hypervisor要做到这一点，需要一些操作系统层面的管理组件，例如memory manager, process scheduler, input/output (I/O) stack, device drivers, security manager, a network stack等。</p><h2 id="Hypervisor的类型"><a href="#Hypervisor的类型" class="headerlink" title="Hypervisor的类型"></a>Hypervisor的类型</h2><p>总的来说，hypervisor可以分为两种类型，type 1 hypervisor和type 2 hypervisor，如下图所示。</p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/03101021997.png" style="zoom: 33%;" /><h3 id="Type-1-hypervisor-Bare-metal"><a href="#Type-1-hypervisor-Bare-metal" class="headerlink" title="Type 1 hypervisor - Bare-metal"></a>Type 1 hypervisor - Bare-metal</h3><p>第一种类型的hypervisor叫做Bare-metal hypervisor（裸金属），直接运行宿主机的硬件之上来管理guest os（虚拟子机的操作系统），代替了宿主机本身操作系统的功能，直接给虚拟机调度规划硬件资源。在企业的IDC中，使用这种类型的hypervisor技术最为常见。</p><p>下面列举一些主流常见的type 1 hypervisor实现：</p><ul><li>KVM（合入了Linux的2007发行版内核中，如果你正在使用最近的Linux，那么你已经拥有使用KVM的能力了:) ）</li><li>Microsoft Hyper-V</li><li>VMware vSphere</li></ul><h3 id="Type-2-hypervisor-Hosted"><a href="#Type-2-hypervisor-Hosted" class="headerlink" title="Type 2 hypervisor - Hosted"></a>Type 2 hypervisor - Hosted</h3><p>第二种类型的hypervisor是Hosted类型的，就称之为托管型的hypervisor吧。这种hypervisor作为软件应用层运行在普通的操作系统之上，来虚拟化出多个guest os给子机使用，因此虚拟子机的资源规划是经过宿主机的操作系统再到底层硬件资源的，跟type 1 hypervisor相比，它需要多经过宿主机的操作系统。这种类型的hypervisor一般是个人用户使用较多，使用起来比较方便。</p><p>下面列举一些主流常见的type 2 hypervisor实现：</p><ul><li>VMware Workstation</li><li>Oracle VirtualBox</li></ul><h2 id="KVM介绍"><a href="#KVM介绍" class="headerlink" title="KVM介绍"></a>KVM介绍</h2><p>KVM是目前最为流行的一种开源的type 1 hypervisor技术，在2006年第一次发布，并在一年后正式合入了Linux内核版本中。KVM已经成为了Linux中的一部分，KVM能让我们把Linux当做hypervisor运行在宿主机之上，虚拟化出多个独立运行的虚拟计算环境，也就是常说的虚拟机了。</p><p>正如前文所说，所有的hypervisor都需要一些操作系统层面的组件来运行虚拟机（memory manager, process scheduler, input/output (I/O) stack, device drivers, security manager, a network stack）。而KVM已经是Linux内核的一部分，那么它天然地就已经具有这种组件和能力了。这些KVM之上的虚拟机，都被实现为普通的Linux进程，被Linux调度器调度，使用专有的虚拟化硬件设备（CPU/显卡/内存/磁盘等）。</p><p>值得一提的是，AWS开源的专门用于容器场景的虚拟化技术Firecracker，正是使用KVM相关技术来创建microVM的。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.redhat.com/en/topics/virtualization/what-is-a-hypervisor">https://www.redhat.com/en/topics/virtualization/what-is-a-hypervisor</a></li><li><a href="https://www.vmware.com/topics/glossary/content/hypervisor.html?resource=cat-1299087558#cat-1299087558">https://www.vmware.com/topics/glossary/content/hypervisor.html?resource=cat-1299087558#cat-1299087558</a></li><li><a href="https://www.redhat.com/en/topics/virtualization/what-is-KVM">https://www.redhat.com/en/topics/virtualization/what-is-KVM</a></li><li><a href="https://www.redhat.com/en/topics/containers/containers-vs-vms">https://www.redhat.com/en/topics/containers/containers-vs-vms</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>虚拟化</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes中的Static pod实现与使用场景分析</title>
    <link href="/2021/06/22/Kubernetes%E4%B8%AD%E7%9A%84static%20pod%E5%AE%9E%E7%8E%B0%E4%B8%8E%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90/"/>
    <url>/2021/06/22/Kubernetes%E4%B8%AD%E7%9A%84static%20pod%E5%AE%9E%E7%8E%B0%E4%B8%8E%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>介绍Kubernetes中的staic pod具体的实现逻辑，以及可能的使用场景。</p><span id="more"></span><h2 id="先决知识"><a href="#先决知识" class="headerlink" title="先决知识"></a>先决知识</h2><ul><li>Kubernetes集群的<a href="https://kubernetes.io/docs/concepts/overview/">基本架构</a>和<a href="https://kubernetes.io/docs/concepts/overview/components/">各组件的功能</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/pods/">Pod</a>的基本概念及原理</li><li>Kubernetes中的<a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a>和<a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">Daemonset</a>基本原理</li></ul><h2 id="Static-pod简介"><a href="#Static-pod简介" class="headerlink" title="Static pod简介"></a>Static pod简介</h2><h3 id="普通的Pod"><a href="#普通的Pod" class="headerlink" title="普通的Pod"></a>普通的Pod</h3><p>Pod是kubernets中最基本的工作单元，一个Pod中可以包含一组容器。通常情况，Pod的创建流程是如下所示（以Bare Pod为例）：</p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/20210707003849.png" style="zoom:50%;" /><p>用户首先是请求Kube-apiserver创建Pod，当Pod被系统接受后，Pod作为一个资源对象被持久化在Etcd中，状态为Pending。控制组件Kube-scheduler通过Kube-apiserver监听到这个未被指定调度节点的Pod后，将会根据一定策略，修改Pod的status字段，标记为这个Pod理应分配到某个节点。而所有节点上的Kubelet也会通过监听Kube-apiserver来发先被调度到本节点Pod，此时Kubelet才真正意义上地根据Pod定义的信息，来在自身节点上启动Pod。</p><h3 id="Static-pod"><a href="#Static-pod" class="headerlink" title="Static pod"></a>Static pod</h3><p>跟Kubernetes中其他普通的Pod不一样，Static pod是直接由节点上的Kubelet管理的。只要把Pod的定义声明文件放在Kubelet所在节点的指定路径下，或者某个指定的URL地址，Kubelet就会读取Pod的定义文件，并且启动这个Pod，也会按照定义的配置管理Static pod的生命周期。Static pod的启动可以不需要集群，只节点上有Kubelet和相应容器运行时即可。</p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/20210707005243.png" style="zoom:50%;" /><h4 id="快速使用Static-pod示例"><a href="#快速使用Static-pod示例" class="headerlink" title="快速使用Static pod示例"></a>快速使用Static pod示例</h4><p>Static pod的使用很简单，我们来快速试用一下吧。</p><ul><li><p>Step 1：准备一台服务器作为节点；</p></li><li><p>Step 2：在节点上安装容器运行时：<a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">点我参考</a>；</p></li><li><p>Step 3：在节点上安装Kubelet：<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl">点我参考</a>；</p></li><li><p>Step 4：手动指定Kubelet启动参数：<br>kubelet –cgroup-driver=systemd –pod-manifest-path=/etc/kubernetes/manifests/ –fail-swap-on=false –pod-infra-container-image=kubernetes/pause &gt; /tmp/kubelet.log 2&gt;&amp;1</p></li><li><p>Step 5：观察Pod定义的容器组已被Kubelet正确启动，纳入管理:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; /etc/kubernetes/manifests/pod.yaml &lt;&lt;EOF<br>&#123;<br>    &quot;kind&quot;:&quot;Pod&quot;,<br>    &quot;apiVersion&quot;:&quot;v1&quot;,<br>    &quot;metadata&quot;:&#123;<br>        &quot;name&quot;:&quot;static-pod-demo&quot;,<br>        &quot;namespace&quot;:&quot;default&quot;,<br>        &quot;uid&quot;:&quot;114b565b-fbe0-4301-a7bd-89598ce86a7a&quot;<br>    &#125;,<br>    &quot;spec&quot;:&#123;<br>        &quot;containers&quot;:[<br>            &#123;<br>                &quot;name&quot;:&quot;nginx-container&quot;,<br>                &quot;image&quot;:&quot;nginx:latest&quot;<br>            &#125;<br>        ],<br>        &quot;restartPolicy&quot;:&quot;Always&quot;<br>    &#125;<br>&#125;<br>EOF<br></code></pre></td></tr></table></figure><p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/20210707122532.png"></p></li><li><p>Step 6：停止Static pod，只需要把Pod.yaml文件移出/etc/kubernetes/manifests/目录即可<br><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/20210707122852.png"></p></li></ul><h2 id="为什么需要Static-pod"><a href="#为什么需要Static-pod" class="headerlink" title="为什么需要Static pod"></a>为什么需要Static pod</h2><p>Kubernetes官方文档，在介绍Static pod时，特别做了如下的标注说明：</p><blockquote><p><strong>Note:</strong> If you are running clustered Kubernetes and are using static Pods to run a Pod on every node, you should probably be using a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset">DaemonSet</a> instead.</p></blockquote><p>也就是说，如果你在使用Static pod来实现kubernetes集群中每个Node的上启动Pod，那么你更应该使用DaemonSet，而不是Static pod。</p><p>既然官方文档都推荐使用DaemonSet了，为什么还存在static pod这种机制？</p><p>早期的Kubernetes，为了在集群各个节点上启动例如日志采集（fluentd）、网络组件(kube-proxy)等服务，使用了Static pod的机制。后来提出了DaemonSet的概念，于是这些需要在每个节点上都启动的服务，都逐渐被DaemonSet所替代，官方文档也建议优先选用DaemonSet。</p><p>Static pod机制一直保留下来，一方面是为了兼容广大开发者采用了Static pod的使用场景；另一方面则是Static pod具有DaemonSet无法替代的特性：不需要Kubernetes集群来直接启动、管理Pod。Static pod最大的特点是无需调用Kube-apiserver即可快速启动Pod，也就是说，不需要一个完整的kubernetes集群，只要安装了kubelet、容器运行时，即可快速地让kubelet来接管你的yaml文件定义的Pod。前面章节也简单介绍了如何快速使用这一特性。Static pod优点是不需要集群，缺点也是相对应的，没有集群层面的管理、调度功能。而Static pod最经典的使用场景，就是用来Bootstrap一个Kubernetes集群。</p><p>我们接着先简单分析Static pod机制在kubernetes中的源码实现，再来分析使用Static pod来Bootstrap Kubernetes集群的过程。</p><h2 id="Static-pod的实现分析"><a href="#Static-pod的实现分析" class="headerlink" title="Static pod的实现分析"></a>Static pod的实现分析</h2><p>下面基于Kubernetes v1.21.2的代码，主要分析Kubelet中处理Static pod的逻辑。<strong>如果对源码实现不感兴趣的读者可以跳过这部分</strong>。</p><blockquote><p><strong>注意</strong>：这里不会详细介绍Kubelet的启动已经各种其他能力，可以参考：</p></blockquote><p>Kubernetes/cmd/kubelet/app/server.go:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">run</span><span class="hljs-params">(ctx context.Context, s *options.KubeletServer, kubeDeps *kubelet.Dependencies, featureGate featuregate.FeatureGate)</span> <span class="hljs-params">(err error)</span></span> &#123;<br>...<br>  <span class="hljs-comment">// About to get clients and such, detect standaloneMode</span><br>    standaloneMode := <span class="hljs-literal">true</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(s.KubeConfig) &gt; <span class="hljs-number">0</span> &#123;<br>      standaloneMode = <span class="hljs-literal">false</span><br>    &#125;<br>...<br>  <span class="hljs-comment">// if in standalone mode, indicate as much by setting all clients to nil</span><br>    <span class="hljs-keyword">switch</span> &#123;<br>    <span class="hljs-keyword">case</span> standaloneMode:<br>      kubeDeps.KubeClient = <span class="hljs-literal">nil</span><br>      kubeDeps.EventClient = <span class="hljs-literal">nil</span><br>      kubeDeps.HeartbeatClient = <span class="hljs-literal">nil</span><br>      klog.InfoS(<span class="hljs-string">&quot;Standalone mode, no API client&quot;</span>)<br>  ...<br>    &#125;<br>  ...<br>  <span class="hljs-keyword">if</span> err := RunKubelet(s, kubeDeps, s.RunOnce); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>  ...<br>&#125;  <br></code></pre></td></tr></table></figure><p>首先找到Kubelet的启动入口，会判断是否传入kubeConfig，如果没有，则是Stand alone模式，即无集群模式，因为没有kubeConfig的Kubelet无法访问某个集群的Kube-apiserver。KubeClient, EeventClient, HeartClient都置为nil，不会上报事件、心跳。</p><p>Kubernetes/cmd/kubelet/app/server.go:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// RunKubelet is responsible for setting up and running a kubelet.  It is used in three different applications:</span><br><span class="hljs-comment">//   1 Integration tests</span><br><span class="hljs-comment">//   2 Kubelet binary</span><br><span class="hljs-comment">//   3 Standalone &#x27;kubernetes&#x27; binary</span><br><span class="hljs-comment">// Eventually, #2 will be replaced with instances of #3</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">RunKubelet</span><span class="hljs-params">(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce <span class="hljs-keyword">bool</span>)</span> <span class="hljs-title">error</span></span> &#123;<br>...<br>  k, err := createAndInitKubelet(<br>    ...<br>  )<br>  ...<br>  startKubelet(k, podCfg, &amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer)<br>  ...<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">createAndInitKubelet</span><span class="hljs-params">(</span></span><br><span class="hljs-function"><span class="hljs-params">  ...</span></span><br><span class="hljs-function"><span class="hljs-params">)</span></span>&#123;<br>  ...<br>  k, err = kubelet.NewMainKubelet(<br>    ...<br>&#125;<br><br><span class="hljs-comment">// NewMainKubelet instantiates a new Kubelet object along with all the required internal modules.</span><br><span class="hljs-comment">// No initialization of Kubelet and its modules should happen here.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">NewMainKubelet</span><span class="hljs-params">(</span></span><br><span class="hljs-function"><span class="hljs-params">  ...</span></span><br><span class="hljs-function"><span class="hljs-params">)</span></span>&#123;<br>  ...<br>kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, nodeHasSynced)<br>   ...<br>&#125;<br></code></pre></td></tr></table></figure><p>RunKubelet -&gt; createAndInitKubelet -&gt; NewMainKubelet. 在NewMainKubelet中，调用makePodSourceConfig来根据kubeletConfiguration，生成PodConfig，一个struct，能够获取多种取到的Pod配置来源（StaticPodPath/StaticPodURL/Kube-apisever)，并能够分发给不通的Listener处理。PodConfig在源码中的注释说明原文如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">//</span> PodConfig is a configuration mux that merges many sources of pod configuration into a single<br><span class="hljs-regexp">//</span> consistent structure, and then delivers incremental change notifications to listeners<br><span class="hljs-regexp">//</span> <span class="hljs-keyword">in</span> order.<br></code></pre></td></tr></table></figure><p>Kubernetes/pkg/kubelet/kubelet.go</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// makePodSourceConfig creates a config.PodConfig from the given</span><br><span class="hljs-comment">// KubeletConfiguration or returns an error.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">makePodSourceConfig</span><span class="hljs-params">(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, nodeName types.NodeName, nodeHasSynced <span class="hljs-keyword">func</span>()</span> <span class="hljs-title">bool</span>) <span class="hljs-params">(*config.PodConfig, error)</span></span> &#123;<br>...<br>  <span class="hljs-comment">// define file config source</span><br><span class="hljs-keyword">if</span> kubeCfg.StaticPodPath != <span class="hljs-string">&quot;&quot;</span> &#123;<br>klog.InfoS(<span class="hljs-string">&quot;Adding static pod path&quot;</span>, <span class="hljs-string">&quot;path&quot;</span>, kubeCfg.StaticPodPath)<br>config.NewSourceFile(kubeCfg.StaticPodPath, nodeName, kubeCfg.FileCheckFrequency.Duration, cfg.Channel(kubetypes.FileSource))<br>&#125;<br><br><span class="hljs-comment">// define url config source</span><br><span class="hljs-keyword">if</span> kubeCfg.StaticPodURL != <span class="hljs-string">&quot;&quot;</span> &#123;<br>klog.InfoS(<span class="hljs-string">&quot;Adding pod URL with HTTP header&quot;</span>, <span class="hljs-string">&quot;URL&quot;</span>, kubeCfg.StaticPodURL, <span class="hljs-string">&quot;header&quot;</span>, manifestURLHeader)<br>config.NewSourceURL(kubeCfg.StaticPodURL, manifestURLHeader, nodeName, kubeCfg.HTTPCheckFrequency.Duration, cfg.Channel(kubetypes.HTTPSource))<br>&#125;<br><br><span class="hljs-keyword">if</span> kubeDeps.KubeClient != <span class="hljs-literal">nil</span> &#123;<br>klog.InfoS(<span class="hljs-string">&quot;Adding apiserver pod source&quot;</span>)<br>config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, nodeHasSynced, cfg.Channel(kubetypes.ApiserverSource))<br>&#125;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>再看看makePodSourceConfig函数里面，就是有三种来源的Pod定义信息，分别是StaticPodPath、StaticPodURL和kubeClient。</p><p>再直接查看kubernetes/pkg/kubelet/config/file.go：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs go">...<br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *sourceFile)</span> <span class="hljs-title">run</span><span class="hljs-params">()</span></span> &#123;<br>listTicker := time.NewTicker(s.period)<br><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>...<br><span class="hljs-keyword">for</span> &#123;<br><span class="hljs-keyword">select</span> &#123;<br><span class="hljs-keyword">case</span> &lt;-listTicker.C:<br><span class="hljs-keyword">if</span> err := s.listConfig(); err != <span class="hljs-literal">nil</span> &#123;<br>klog.ErrorS(err, <span class="hljs-string">&quot;Unable to read config path&quot;</span>, <span class="hljs-string">&quot;path&quot;</span>, s.path)<br>&#125;<br>...<br>&#125;<br>&#125;<br>&#125;()<br><br>s.startWatch()<br>&#125;<br>...<br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *sourceFile)</span> <span class="hljs-title">listConfig</span><span class="hljs-params">()</span> <span class="hljs-title">error</span></span> &#123;<br>...<br><span class="hljs-keyword">switch</span> &#123;<br><span class="hljs-keyword">case</span> statInfo.Mode().IsDir():<br>...<br><span class="hljs-keyword">case</span> statInfo.Mode().IsRegular():<br>...<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到如果StaticPodPath配置了的话，kubelet会启动一个协程，定时watch这个路径下的所有文件，如果是符合Pod定义的文件，则把反序列化出来的Pod对象分发给PodConfig的Listener进行处理。</p><p>Stuct PodConfig的成员updates chan kubetypes.PodUpdate是一个分发需要处理的Pod事件的Channel。</p><p>Kubernetes/pkg/kubelet/kubelet.go</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kl *Kubelet)</span> <span class="hljs-title">syncLoopIteration</span><span class="hljs-params">(configCh &lt;-<span class="hljs-keyword">chan</span> kubetypes.PodUpdate, handler SyncHandler,</span></span><br><span class="hljs-function"><span class="hljs-params">syncCh &lt;-<span class="hljs-keyword">chan</span> time.Time, housekeepingCh &lt;-<span class="hljs-keyword">chan</span> time.Time, plegCh &lt;-<span class="hljs-keyword">chan</span> *pleg.PodLifecycleEvent)</span> <span class="hljs-title">bool</span></span> &#123;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>在函数syncLoopIteration中，通过读取PodConfig的updates这个Channel，来分发出事件进行处理，主要是由Kubelet按照Pod的定义，调用底层的容器运行时来运行容器。当然，当Kubelet watch到指定路径的Pod定义文件被移除，那它也会停止原本运行的Static pod。后面的实现代码边幅较长不再详细列举。</p><h2 id="Static-pod的典型应用场景"><a href="#Static-pod的典型应用场景" class="headerlink" title="Static pod的典型应用场景"></a>Static pod的典型应用场景</h2><p>Static pod目前使用最广泛的场景，是在Kubeadm中使用使用这一机制来Bootstrap一个Kubernetes集群。</p><p>使用Kubernetes集群前，需要把管控面的组件先部署好。这些管控组件可以二进制部署，也可以容器化部署。二进制部署的方式稍显繁琐，且容易出错，升级也不方便，容器化部署这些管控组件的好处显而易见。</p><p>这是最典型的先有鸡还是先有蛋的问题。在没有Kubernetes集群的时候，我们如何把这些管控组件以容器化的形式启动起来？官方部署工具Kubeadm给出的解决方法就是使用Static pod。</p><p>在使用Kubeadm部署集群时，首先需要安装好kubelet、容器运行时等组件，Kubeadm会根据指定配置文件，生成Kube-apiserver, Kube-controller-manager, Kube-proxy等组件的Pod定义文件，放置在Master节点的指定Static Pod path下，让Kubelet接管这些Static pod的生命周期管理。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了Kubernetes中Static pod机制的原理，并与“常规的”集群中运行的Pod进行了对比。然后说明了如何快速使用Kubelet来部署启动、重启和停止Static pod来实现对一组业务容器的生命周期管理。如果我们需要在Kubernetes中指定每个节点启动特定的Pod，那么建议使用官方的DaemonSet来实现目的。Static pod的使用场景在于无集群时，如何能方便、稳定地管理运行在本节点上的容器。Static pod目前最广泛的应用场景是Kubeadm中利用这一机制来启动Kubernetes集群的Control Plane层面的各个组件。</p><p>Static pod仅仅依赖所在节点的Kubelet即可启动，不需要集群，但也缺少了集群层面的资源调度、自动伸缩等功能。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://stackoverflow.com/questions/59612514/whats-the-difference-between-pods-and-static-pods-in-kubernetes-and-when-to">stack overflow: whats-the-difference-between-pods-and-static-pods</a></li><li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/">https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/</a></li><li><a href="https://octetz.com/docs/2019/2019-10-12-static-pods/">https://octetz.com/docs/2019/2019-10-12-static-pods/</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Ploto quick start</title>
    <link href="/2021/04/13/ploto-quick-start/"/>
    <url>/2021/04/13/ploto-quick-start/</url>
    
    <content type="html"><![CDATA[<p>介绍任务调度与执行框架Ploto的quick start. 目前可部署使用，源码待整理后开源。</p><span id="more"></span><h2 id="Ploto整体架构"><a href="#Ploto整体架构" class="headerlink" title="Ploto整体架构"></a>Ploto整体架构</h2><p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/hVLQ7zxmGJ2l_ltV1vzQMA.png" alt="img">        </p><h2 id="本文档目标"><a href="#本文档目标" class="headerlink" title="本文档目标"></a>本文档目标</h2><p>本文不会介绍Ploto的具体实现，而是介绍如何快速上手使用Ploto完成如下事项：</p><ul><li>在Kubernetes集群中部署Ploto任务调度与执行框架</li><li>创建Task, DynamicExecutorPool自定义资源定义（Custom Resource Definition）</li><li>创建一系列的Task实例，和一个示例DynamicExecutorPool，然后ploto-controller会把这些task分配给DynamicExecutorPool管理的裸pod，pod中的应用容器消费任务（本例为打印task中的para）</li></ul><h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><ul><li>可用的kubernetes集群，建议版本&gt;=1.16.3</li></ul><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="1-下载ploto-demo项目"><a href="#1-下载ploto-demo项目" class="headerlink" title="1. 下载ploto-demo项目"></a>1. 下载ploto-demo项目</h3><p>ploto-demo中保存了本示例所需的yaml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone https://github.com/xawei/ploto-demo.git<br>cd ploto-demo<br></code></pre></td></tr></table></figure><h3 id="2-创建CRD和新建命名空间"><a href="#2-创建CRD和新建命名空间" class="headerlink" title="2. 创建CRD和新建命名空间"></a>2. 创建CRD和新建命名空间</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl apply -f manifest/crds/ploto.io_dynamicexecutorpools.yaml<br>kubectl apply -f manifest/crds/ploto.io_fixedexecutorpools.yaml<br>kubectl apply -f manifest/crds/ploto.io_tasks.yaml<br>kubectl create ns ploto-system<br>kubectl create ns ploto-demo<br></code></pre></td></tr></table></figure><p>查看CRD：</p><p><img src="https://weiblog.oss-cn-beijing.aliyuncs.com/img/20201130163857.png"></p><h3 id="3-创建默认资源池扩缩容算法的Webhook"><a href="#3-创建默认资源池扩缩容算法的Webhook" class="headerlink" title="3. 创建默认资源池扩缩容算法的Webhook"></a>3. 创建默认资源池扩缩容算法的Webhook</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl apply -f manifest/webhook/autoscaler-webhook-service.yaml<br></code></pre></td></tr></table></figure><h3 id="4-创建Ploto-Controller"><a href="#4-创建Ploto-Controller" class="headerlink" title="4. 创建Ploto Controller"></a>4. 创建Ploto Controller</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl apply -f manifest/controllers/ploto-controller.yaml<br></code></pre></td></tr></table></figure><p>查看ns ploto-system下的资源：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get all -n ploto-system<br></code></pre></td></tr></table></figure><p><img src="https://weiblog.oss-cn-beijing.aliyuncs.com/img/20201130164726.png"></p><p>至此，我们已经完成了在kubernetes至少上部署了Ploto框架所必须的预备资源。下面再继续创建与业务相关的Task和DynamicExecutorPool自定义资源。</p><h3 id="5-创建自定义的Dynamic-Executor-Pool"><a href="#5-创建自定义的Dynamic-Executor-Pool" class="headerlink" title="5. 创建自定义的Dynamic Executor Pool"></a>5. 创建自定义的Dynamic Executor Pool</h3><p>准备创建一个DynamicExecutorPool，先查看一下我们定义的dep1.yaml：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat manifest/deps/dep-simple-logger.yaml<br></code></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># manifest/deps/dep-simple-logger.yaml</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">&quot;ploto.io/v1alpha1&quot;</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">DynamicExecutorPool</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">dep-simple-logger</span><br>  <span class="hljs-attr">namespce:</span> <span class="hljs-string">ploto-demo</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">initialExecutor:</span> <span class="hljs-number">2</span> <span class="hljs-comment"># executor资源池的初始pod（执行器）数量</span><br>  <span class="hljs-attr">maxExecutor:</span> <span class="hljs-number">5</span> <span class="hljs-comment"># executor资源池的最大pod（执行器）数量</span><br>  <span class="hljs-attr">minExecutor:</span> <span class="hljs-number">2</span> <span class="hljs-comment"># executor资源池的最小pod（执行器）数量</span><br>  <span class="hljs-comment"># 动态扩缩容的webhook，可选，默认预估下一周所需executor pod数量为</span><br>  <span class="hljs-comment"># 本周期在执行任务的executor pod数量的1.3倍</span><br>  <span class="hljs-attr">webhook:</span> <br>    <span class="hljs-attr">service:</span><br>      <span class="hljs-attr">namespace:</span> <span class="hljs-string">ploto-system</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">autoscaler-webhook-service</span><br>      <span class="hljs-attr">path:</span> <span class="hljs-string">/scale</span><br>      <span class="hljs-attr">port:</span> <span class="hljs-number">8000</span><br>  <span class="hljs-attr">container:</span> <span class="hljs-comment"># 应用容器，消费task中的para信息</span><br>    <span class="hljs-attr">command:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">/app/bin/simple-logger</span><br>    <span class="hljs-attr">args:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;--simulate-exe-time=60&quot;</span> <span class="hljs-comment"># 应用容器可指定模拟任务执行的耗时，再打印task信息</span><br>    <span class="hljs-attr">env:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">PATH</span><br>        <span class="hljs-attr">value:</span> <span class="hljs-string">/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">ccr.ccs.tencentyun.com/ploto/simple-logger:1.0</span><br>    <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">Always</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">simple-logger</span><br>    <span class="hljs-attr">resources:</span><br>      <span class="hljs-attr">limits:</span><br>        <span class="hljs-attr">cpu:</span> <span class="hljs-string">500m</span><br>        <span class="hljs-attr">memory:</span> <span class="hljs-string">1Gi</span><br>      <span class="hljs-attr">requests:</span><br>        <span class="hljs-attr">cpu:</span> <span class="hljs-string">250m</span><br>        <span class="hljs-attr">memory:</span> <span class="hljs-string">256Mi</span><br>    <span class="hljs-attr">securityContext:</span><br>      <span class="hljs-attr">privileged:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">terminationMessagePath:</span> <span class="hljs-string">/dev/termination-log</span><br>    <span class="hljs-attr">terminationMessagePolicy:</span> <span class="hljs-string">File</span><br>    <span class="hljs-attr">workingDir:</span> <span class="hljs-string">/</span><br></code></pre></td></tr></table></figure><p>其中定义的container是我们的应用容器，持续监听本地任务文件，一旦任务文件存在，则会去读Task.Spec.Para中的数据，执行任务。</p><p>创建这个dynamic executor pool（简称dep）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl apply -f manifest/deps/dep-simple-logger.yaml<br></code></pre></td></tr></table></figure><p>可以查看目前dep的状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get dep -o wide -n ploto-demo<br>kubectl get pod -n ploto-demo<br></code></pre></td></tr></table></figure><p><img src="https://weiblog.oss-cn-beijing.aliyuncs.com/img/20201202220002.png"></p><p>dep-simple-logger刚创建时，还没有executor pod，当ploto-controller监听到dep-simple-logger时，则为这个dep创建了2个pod（ initialExecutor: 2）。</p><h3 id="6-创建task，并查看执行情况"><a href="#6-创建task，并查看执行情况" class="headerlink" title="6. 创建task，并查看执行情况"></a>6. 创建task，并查看执行情况</h3><p>manifest/tasks/中准备了6个task，我们查看其中一个task1.yaml，下面附上了注释：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">cat manifest<span class="hljs-regexp">/tasks/</span>task1.yaml<br></code></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">&quot;ploto.io/v1alpha1&quot;</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Task</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">task1</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">executorPool:</span> <span class="hljs-string">dep-simple-logger</span> <span class="hljs-comment"># 指定消费这个task的executor pod所属的资源池</span><br>  <span class="hljs-attr">executorPoolType:</span> <span class="hljs-string">Dynamic</span> <span class="hljs-comment"># 指定消费这个task的executor pod所属的资源池类型</span><br>  <span class="hljs-attr">para:</span> <span class="hljs-string">https://ploto.io/tasks/1.html</span> <span class="hljs-comment"># task携带的任务信息</span><br></code></pre></td></tr></table></figure><p>创建这些task：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl apply -f manifest/tasks/<br></code></pre></td></tr></table></figure><p>如上图所示，6个新创建的task，初始状态为Pending，等待调度关联到对应的executorPool管理的pod，由pod执行消费任务。</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs maxima">kubectl <span class="hljs-built_in">get</span> task -n ploto-<span class="hljs-built_in">demo</span><br>kubectl <span class="hljs-built_in">get</span> pod -n ploto-<span class="hljs-built_in">demo</span><br></code></pre></td></tr></table></figure><p><img src="https://weiblog.oss-cn-beijing.aliyuncs.com/img/20201202220221.png"></p><p>task状态为Running后，等待manifest/deps/dep1.yaml中定义的simulate-exe-time启动参数值（单位秒）的时间后，可以看到部分task执行完成了（status: completed）：</p><p><img src="https://weiblog.oss-cn-beijing.aliyuncs.com/img/20201202220326.png"></p><p>查看对应的executor pod，可以看到输出（消费task）：</p><p><img src="https://weiblog.oss-cn-beijing.aliyuncs.com/img/20201202220655.png"></p><p>本例executor pod为简单地监听分配到的task任务，并打印task中的para参数。</p><p>ploto-controller会根据task的数量、执行情况，周期性地对dep管理的executor pod资源数量，在[minExecutor, maxExecutor]区间内做动态的扩缩容。默认为每一个周期需要的pod数量为上一周期的1.3倍。你也可以实现自定义的扩缩容webhook，在dep中配置。</p><p>等待所有任务执行完后，我们查看dep的状态，以及它所关联的pod。发现pod数量已缩容至dep-simple-loger实例中设置的最小值minExexutor: 2。</p><p><img src="https://weiblog.oss-cn-beijing.aliyuncs.com/img/20201202220809.png"></p><h3 id="附：-清理ploto相关资源"><a href="#附：-清理ploto相关资源" class="headerlink" title="附： 清理ploto相关资源"></a>附： 清理ploto相关资源</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl delete -f manifest/crds/<br>kubectl delete -f manifest/sa/<br>kubectl delete -f manifest/controllers/<br><br>kubectl delete ns ploto-system<br>kubectl delete ns ploto-demo<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>开源</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>使用CRD和Aggregated Server扩展Kubernetes</title>
    <link href="/2021/03/05/%E4%BD%BF%E7%94%A8CRD%E5%92%8CAggregated%20Server%E6%89%A9%E5%B1%95Kubernetes/"/>
    <url>/2021/03/05/%E4%BD%BF%E7%94%A8CRD%E5%92%8CAggregated%20Server%E6%89%A9%E5%B1%95Kubernetes/</url>
    
    <content type="html"><![CDATA[<p>Kubernetes官方提供了两种常用的方式让开发者可以方便地进行自定义的扩展。一种是使用CRD（Custom Resource Definition），另一种则是配置Aggregation layer，将请求代理到Aggreated Server中。本文主要介绍两者的使用原理和相关的适用场景。</p><span id="more"></span><h2 id="CRD扩展原理简介"><a href="#CRD扩展原理简介" class="headerlink" title="CRD扩展原理简介"></a>CRD扩展原理简介</h2><p>在Kube-apisever中，有许多内置的API对象，例如我们非常熟悉的Pod、ConfigMap、Namespace等。Kube-apiserver能够识别这些API对象，并把相关的资源实例持久化存储在集群的etcd中。所以当我们使用<code>kubectl get node</code>等命令时，Kube-apiserver能够正确处理，返回相应的信息。</p><p>如果我们需要让Kube-apiserver认识一些自定义的对象，例如nodePool，并且通过<code>kubectl get nodepool</code>能够输出相关的信息，该怎么办呢？Kubernetes官方提供的CRD特性，让我们能够自定义API对象来扩展Kube-apiserver。</p><p>首先，需要在Kubernetes中创建CRD模板对象。即告诉Kubernetes，我现在需要一种自定义的对象，它带有哪些字段、哪些字段是必填或可选、字段的值应该是什么类型、通过kubectl查看时应该如何输出等。这时，Kube-apiserver就已经理解了你需要自定义的资源模板了。CRD相当于一个类的模板。</p><p>然后，当需真正创建了自定义资源实例CR后，对应的信息就会持久化存储在后端的etcd中，我们可以通过kubectl命令查看到实例的信息。而CRD一般会配套对应的Controller，监听自定义资源的创建、更新、删除等，由Controller再进行实际业务逻辑的处理。</p><h2 id="Aggreated-Server扩展原理简介"><a href="#Aggreated-Server扩展原理简介" class="headerlink" title="Aggreated Server扩展原理简介"></a>Aggreated Server扩展原理简介</h2><p>使用Aggregation Layer，可以在Kubernetes的核心API之外，进行额外的扩展。例如 <a href="https://github.com/kubernetes-sigs/metrics-server">metrics server</a>，就是在一种现成的扩展方案。当然我们也可以自行开发需要扩展的API。CRD的方案，是让kube-apiserver识别更多类型的对象，而Aggregation layer则不一样，它是在Kube-apiserver进程中一起运行的，需要注册扩展的资源才能发挥功效。</p><p>通过APIService对象可以注册API：使用APIService声明一个在Kubernetes API中的URL路径。这样，当访问这个URL路径时，Kube-apiserver中的Aggregation layer就会把请求代理转发到对应的APIService对象。</p><p>最常见的使用APService的方式，是在Kubernetes集群中启动Pod(s)来作为扩展的API Server。而一般情况下，使用Aggreated  Server来管理资源，都会配套相应的一个或多个Controller在集群中。使用apiserver-builder库可以生成扩展的Aggreated Server和对应Controller的基本工程框架。</p><p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/03101324436.png"></p><h2 id="使用场景分析"><a href="#使用场景分析" class="headerlink" title="使用场景分析"></a>使用场景分析</h2><h3 id="CRD"><a href="#CRD" class="headerlink" title="CRD"></a>CRD</h3><p>在kubernetes中管理有状态应用，是比较复杂的，而一个相对灵活、编程友好的管理“有状态应用”的解决方案就是Operator，依赖CRD和Controller进行实现。</p><p>Operator的工作原理，实际上是利用Kubernetes的CRD来描述我们想要部署的“有状态应用”或想要得到的资源，然后再自定义Controller中根据自定义API对象的变化，完成具体的部署和维护工作。</p><p>例如Kubernetes的子项目Cluster API，也是使用Operator模式，在Management Cluster中通过应用CRD来定义集群、机器等资源模板，额外部署的各种Controller，则是负责读取相关的CRD实例信息，对集群、机器等资源进行生命周期的管理，包括创建、更新、删除等。</p><p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/03101325491.png"></p><h3 id="Aggregated-Server"><a href="#Aggregated-Server" class="headerlink" title="Aggregated Server"></a>Aggregated Server</h3><p>一个典型的通过APIService方式配置Aggregated Server来扩展Kube-apisever的项目，则是Metric server了。</p><p>Metric server采集进行autoscaling所需的指标数据：CPU &amp; Memory。Metric server本身并不会计算指标数值，而是Kubelet计算的。Metric server采集Kubelet所暴露的指标数据，进行聚集后，通过API的形式暴露出来，进行autoscaling。</p><p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/03101327905.png"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/">https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/</a></li><li><a href="https://kubernetes.io/docs/tasks/extend-kubernetes/setup-extension-api-server/">https://kubernetes.io/docs/tasks/extend-kubernetes/setup-extension-api-server/</a></li><li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/</a></li><li><a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/">https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/</a></li><li><a href="https://cloud.redhat.com/blog/kubernetes-operators-best-practices">https://cloud.redhat.com/blog/kubernetes-operators-best-practices</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>PV和PVC的基本概念和使用</title>
    <link href="/2021/01/10/pv-pvc-basic-use/"/>
    <url>/2021/01/10/pv-pvc-basic-use/</url>
    
    <content type="html"><![CDATA[<p>PV和PVC是kubernetes存储管理中的重要概念，在日常生产场景中使用非常广泛。本文主要介绍PV和PVC在kubernetes中的基本概念、使用场景以及实现原理。更多PV和PVC的使用细节问题请参考kubernetes官方文档。</p><span id="more"></span><h2 id="kubernetes存储中的卷"><a href="#kubernetes存储中的卷" class="headerlink" title="kubernetes存储中的卷"></a>kubernetes存储中的卷</h2><p>容器中的文件在磁盘上是临时存放的，也有很多场景下应用程序都需要对某些数据进行持久存储，避免在容器奔溃时造成数据丢失。</p><p>在kubernetes中，提供了挂载卷（Volume）的能力，卷的类型有很多种，例如还有跟云厂商关联的awsElasticBlockStore、azureDisk、azureFile等，具体可以参考<a href="https://kubernetes.io/docs/concepts/storage/volumes/">官方文档</a>。</p><p>主要的常用卷类型包括：</p><p>emptyDir：卷最初是空的，在pod在节点运行时创建，pod删除时数据也会永久删除；</p><p>configMap：可以将configMap中的数据作为卷挂在到pod中；</p><p>secret：可以将secret中的数据作为卷挂载到pod中；</p><p>downwardAPI：将pod的元数据信息注入到pod中；</p><p>hostPath：能将主机节点文件系统上的文件或目录挂载到 Pod 中；</p><p>nfs：将 NFS (网络文件系统) 挂载到 Pod，可以多挂；</p><p>kubernetes的一个重要的基本理念是：<strong>向应用开发者隐藏真实的基础设施，使他们不需要关心基础设施的具体状况信息，并使应用程序可以在不同的云服务商之前进行迁移、切换</strong>。因此，kubernetes提出了PV和PVC的概念，使开发人员可以在创建pod需要使用持久化存储时，就像请求CPU\MEM等资源一样来向kubernetes集群请求持久存储。</p><h2 id="PV和PVC"><a href="#PV和PVC" class="headerlink" title="PV和PVC"></a>PV和PVC</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>前面提到的emptyDir和hostPath都不是持久化存储，会随着Pod的销毁和重建而丢失。而PV和PVC都是kubernetes中定义的API资源，提供一种能持久化存储的能力。</p><p>PV是集群中的一块存储，一般可以由集群的管理员事先供应，或者使用storage class的方式来动态供应。pv属于集群资源，它们的生命周期跟使用它们的pod时相互独立。</p><p>PVC表达的是用户对存储的请求（persistant volume claim），也是kubernetes中独立存在的API资源。Pod 会耗用节点资源，而 PVC 申领会耗用 PV 资源。Pod 可以请求特定数量的资源（CPU 和内存）；同样PVC也可以请求特定的大小和访问模式。</p><p>当用户创建一个PVC，kubernetes中的volume controller会监测到PVC的对象，寻到集群中与之匹配的PV资源，将二者进行绑定。如果没有匹配的PV资源，PVC则会处理未绑定的状态一直持续等待，直到集群中出现满足条件的PV资源后进行绑定。PVC和PV之间的绑定是一种一对一的映射。</p><p>而另一个很重要的概念，也是kubernetes的API资源，就是storageClass。storageClass可以说是PV的创建模板。前面提到，PV可以是集群管理员事先供应的，就是所谓的静态供应（static provisioning）。这个方法很大的一个问题在于，当kubernetes集群规模很大时，需要管理员手工去创建成千上万的PV来对应存储资源，这是很繁琐的，因此，kubernetes中PV的创建一般会使用动态供应（dynamic provisioning）。</p><p>在storageClass中会定义：（1）PV的属性，如存储类型和大小；（2）创建PV需要的存储插件，如NFS。</p><p>这样，kubernetes就可以根据PVC指定的storageClass，调用指定的存储插件，创建所需的PV。</p><p>Pod、PVC、PV、StorageClass的关系图可以解释如下：</p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/20210511215559.png" style="zoom:67%;" /><h3 id="Pod中使用持久存储"><a href="#Pod中使用持久存储" class="headerlink" title="Pod中使用持久存储"></a>Pod中使用持久存储</h3><p>例如，用户创建一个PVC，如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nfs</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">accessModes:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteMany</span><br>  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">manual</span><br>  <span class="hljs-attr">resources:</span><br>    <span class="hljs-attr">requests:</span><br>      <span class="hljs-attr">storage:</span> <span class="hljs-string">1Gi</span><br></code></pre></td></tr></table></figure><p>集群中的Volume Controller发现这个PVC后，就会主动在集群中寻找合适的PV，来和PVC绑定。只有和PV绑定了的PVC，才能被pod正常挂载使用。Volume Controller寻找PV的条件主要是：<br>（1）PVC和PV的spec字段中指定的规格，例如存储（storage）的大小；</p><p>（2）PVC和PV的storageClassName必须一样。（这里的storage）</p><p> 如果集群中存在类型下面这样能满足PVC条件的PV，则可能会被绑定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolume</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nfs</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">storageclassName:</span> <span class="hljs-string">manual</span><br>  <span class="hljs-attr">capacity:</span><br>    <span class="hljs-attr">storage:</span> <span class="hljs-string">1Gi</span><br>  <span class="hljs-attr">accessModes:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteMany</span><br>  <span class="hljs-attr">nfs:</span><br>    <span class="hljs-attr">server:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;/&quot;</span><br></code></pre></td></tr></table></figure><p>用户可以再在自己Pod中声明使用这个PVC了。为什么Pod使用这个PVC就可以实现容器的持久存储呢？其实容器的Volume就是将一个宿主机上的目录跟一个容器里的目录绑定挂载。只要宿主机上的这个路径的目录是”持久“的，那么在容器中的路径Volume也就是”持久”的了。所谓的持久，就是容器被删除，而Volume可以保留。</p><p>这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理”。</p><p>（1）Attach：为宿主机挂载远程存储；（如果是NFS的话，其实没有这个过程，因为不需要“挂载存储设备到宿主机”）</p><p>（2）Mount：将远程存储格式化挂载到宿主机的指定目录，对应容器中的Volume。</p><p>我们可以创建这样的一个Pod来使用PVC</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">role:</span> <span class="hljs-string">web-frontend</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">web</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span><br>    <span class="hljs-attr">ports:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">web</span><br>        <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span><br>    <span class="hljs-attr">volumeMounts:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nfs</span><br>          <span class="hljs-attr">mountPath:</span> <span class="hljs-string">&quot;/usr/share/nginx/html&quot;</span><br>  <span class="hljs-attr">volumes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nfs</span><br>    <span class="hljs-attr">persistentVolumeClaim:</span><br>      <span class="hljs-attr">claimName:</span> <span class="hljs-string">nfs</span><br></code></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/">https://kubernetes.io/docs/concepts/storage/volumes/</a></li><li><a href="https://time.geekbang.org/column/intro/100015201">https://time.geekbang.org/column/intro/100015201</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>几种常见的发布策略</title>
    <link href="/2020/12/13/%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8F%91%E5%B8%83%E7%AD%96%E7%95%A5/"/>
    <url>/2020/12/13/%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8F%91%E5%B8%83%E7%AD%96%E7%95%A5/</url>
    
    <content type="html"><![CDATA[<p>本文主要介绍Kubernetes中经常使用到的几种发布策略，如蓝绿发布、红黑发布、金丝雀（灰度）发布、滚动发布等。</p><span id="more"></span><h2 id="蓝绿发布"><a href="#蓝绿发布" class="headerlink" title="蓝绿发布"></a>蓝绿发布</h2><p>蓝绿发布，<strong>可将用户流量从先前版本的应用逐渐转移到几乎相同的新版本中（两者均保持在生产环境中运行）</strong>。 旧版本可以称为蓝色环境，而新版本则可称为绿色环境。 一旦生产流量从蓝色完全转移到绿色，蓝色就可以在回滚或退出生产的情况下保持待机，也可以更新成为下次更新的模板。</p><p>蓝绿发布省级和回退速度较快，缺点则是如果v2版本有问题，切回v1版本，此时可用资源减少了一半，一般不适于在业务高峰期使用这种发布策略，以避免可能出现服务过载。</p><p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/20221213010921.png"></p><h2 id="红黑发布"><a href="#红黑发布" class="headerlink" title="红黑发布"></a>红黑发布</h2><p>红黑发布与蓝绿发布有点类似，也是通过两个环境完成软件版本的升级，将当前生产流量指向的环境称为红环境，新版本环境称为黑环境。红黑发布英文叫A/B testing，流程是：</p><ol><li>申请黑色集群Group2，部署新服务；</li><li>负载均衡直向Group2；</li><li>删除Group1</li></ol><p>与蓝绿发布不同，红黑发布在过程中会需要两倍的资源。</p><p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/20221213012144.png"></p><h2 id="金丝雀发布"><a href="#金丝雀发布" class="headerlink" title="金丝雀发布"></a>金丝雀发布</h2><p>所谓金丝雀发布，也即是灰度发布的一种。在发布过程中，生产环境同时存在v1和v2两个版本，一部分用户使用v1，另一部分用户则使用v2，逐渐过渡。</p><p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/20221213012102.png"></p><h2 id="滚动发布"><a href="#滚动发布" class="headerlink" title="滚动发布"></a>滚动发布</h2><p>滚动发布则是在金丝雀发布基础上进行了改进，是一种自动化程度较高的发布方式，在升级过程中，先启动一批次的新版本，再停止旧版本。在确认运行正常后，再继续，直至完成版本的全量更新。</p><p>滚动发布是目前较为主流的发布方式。</p><p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/20221213012220.png"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.redhat.com/zh/topics/devops/what-is-blue-green-deployment">https://www.redhat.com/zh/topics/devops/what-is-blue-green-deployment</a></li><li><a href="https://blog.container-solutions.com/kubernetes-deployment-strategies">https://blog.container-solutions.com/kubernetes-deployment-strategies</a></li><li><a href="https://www.cnblogs.com/hunternet/p/14306105.html">https://www.cnblogs.com/hunternet/p/14306105.html</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes架构及核心组件介绍</title>
    <link href="/2019/10/17/Kubernetes%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D/"/>
    <url>/2019/10/17/Kubernetes%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<p>Kubernets是一个容器编排平台，本文主要介绍其整体架构，以及各个核心组件之间是如何协作完成容器的各项编排工作。</p><span id="more"></span><h2 id="Kubernetes的架构及组件基础概念"><a href="#Kubernetes的架构及组件基础概念" class="headerlink" title="Kubernetes的架构及组件基础概念"></a>Kubernetes的架构及组件基础概念</h2><p>当你完成Kubernets部署的时候，你得到的是一个集群，集群包含控制面（control plane）组件（master部分），以及woker节点。用户的业务应用容器以Pod的形式，运行在这些worker节点之前。而master控制面组件，则负责管理集群中运行woker节点以及各个容器Pod。在生产环境中，控制面组件一般以多副本的形式部署在多台不同的机器上，以保证高可用。</p><p>下面是Kubernets集群的运行架构及核心组件示意图：</p><p><img src="https://weiblog-1252613377.cos.ap-chengdu.myqcloud.com/201912152000007.png"></p><p>如图所示，左边的控制面的组件，包括etcd, controller-manager, kube-scheduler等。而worker节点上也运行着kubelet和kube-proxy组件。下面分别简单介绍各组件的主要功能。</p><h2 id="控制面组件（Control-Plane-Components）"><a href="#控制面组件（Control-Plane-Components）" class="headerlink" title="控制面组件（Control Plane Components）"></a>控制面组件（Control Plane Components）</h2><p>控制面组件执行集群层面的全局操作（如调度）、探测事件并做出相应动作（如当pod副本数不符合deployment中replicas字段设置的值时，启动或停止pod）等。控制面的组件可以在集群中的任意节点上运行，但是一般为了方便管理，一般这些控制面组件会在特定的节点上运行，称之为master节点，不会运行用户的业务容器，与worker节点区分开来。</p><h3 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h3><p>kube-apiserver是Kubernetes集群的一个控制面组件，对外暴露了Kubernetes API。kube-apiserver是一个无状态的件，我们可以水平扩展，部署多个副本，让流量均衡地分发到多个kube-apiserver服务上。</p><h3 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h3><p>etcd是Kubernetes的默认的持久化KV存储后端，存储整个Kuberntes集群的各种状态信息。需要注意备份这些信息。</p><h3 id="kube-scheduler"><a href="#kube-scheduler" class="headerlink" title="kube-scheduler"></a>kube-scheduler</h3><p>Kube-scheduler通过kube-apiserver持续监听集群中新创建还未分配节点的pod，根据一系列策略算法，选择节点来运行pod。策略算法考虑的因素包括：当前pod和其他pod的resource requirements、软硬件、policy限制、亲和性/反亲和性、资源分布性（例如多个pod应该落在多个可用区来增强可用性）等。</p><p>需要注意的是，kube-scheduler是为pod分配节点，在pod资源上进行标记，而未实际把pod启动起来，因为那是kubelet的工作，后面会介绍到。</p><h3 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h3><p>kube-controller-manager是Kubernets中各种内置controller控制器的合集，在逻辑上，每个controller都是独立运行工作的，而实际上为了降低复杂性，这些controller都被编译到一个二进制中，启动一个进程来运行这些controller。一些典型的controller包括：</p><ul><li>Node controller：监测节点的宕机情况，并做出响应（如驱逐宕机节点上的pod）；</li><li>Service controller：为每个新的namespace创建默认的ServiceAccounts</li></ul><h3 id="cloud-controller-manager"><a href="#cloud-controller-manager" class="headerlink" title="cloud-controller-manager"></a>cloud-controller-manager</h3><p>我们可以在Kubernetes中添加特定云服务的控制逻辑。这些cloud controller与Kubernets中只与内部交互的controller不同，会与云厂商的API进行交互，管理云基础设置。</p><h2 id="Worker节点上的组件"><a href="#Worker节点上的组件" class="headerlink" title="Worker节点上的组件"></a>Worker节点上的组件</h2><p>kubelet和kube-proxy运行在每个节点上，负责维护运行的pod和Kubernetes的运行时环境。</p><h3 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h3><p>kueblet是一个运行在节点上的agent，通过kube-apiserver监听分配到该节点上的pod，然后根据pod的定义，调用容器运行时，确保pod的容器业务运行起来。（当然还有特殊的static pod了，可以参考：<a href="http://xawei.xyz/2021/06/22/Kubernetes%E4%B8%AD%E7%9A%84static%20pod%E5%AE%9E%E7%8E%B0%E4%B8%8E%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90/">Kubernetes中的Static pod实现与使用场景分析</a>）</p><h3 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h3><p>Kube-proxy是每个节点上都运行的network proxy，负责实现Kubernetes中的Service的概念。Kube-proxy维护节点上的网络路由规则，让集群上同节点或不同节点的pod可以通信。</p><p>如果操作系统支持包过滤层（packet filtering layer），那么kube-proxy会使用这一方式实现它的功能，否则kube-proxy会自己来进行流量转发。</p><h3 id="Container-runtime"><a href="#Container-runtime" class="headerlink" title="Container runtime"></a>Container runtime</h3><p>容器运行时是负责实际运行容器的组件，如containerd、CRI-O等，任意使用了<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md">Kuberntes CRI协议</a>的服务都可以作为Kuberntes的容器运行时。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://kubernetes.io/docs/concepts/overview/components/">https://kubernetes.io/docs/concepts/overview/components/</a></li><li><a href="https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/">https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
